{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Qadence Model","text":"<p>Qadence Model is a collection of features to enhance Qadence quantum machine learning features</p>"},{"location":"#pre-requisites","title":"Pre-requisites","text":"<p>The library uses the following tools:</p> <ul> <li>hatch for managing virtual environment and dependencies</li> <li>pytest for building the unit tests suite</li> <li>black, isort and flake8 for code formatting and linting</li> <li>mypy for static type checking</li> <li>pre-commit for applying linting and formatting automatically before committing new code</li> </ul> <p>We recommend to use <code>pyenv</code> for managing python versions for managing python versions both globally and locally:</p> <pre><code># System-wide install of a python version.\npyenv install 3.10\n\n# Use 3.10 everywhere.\npyenv global 3.10\n\n# Or locally in the current directory.\npyenv local 3.10\n</code></pre>"},{"location":"#install-from-pypi","title":"Install from PyPi","text":"<p><code>qadence-model</code> is available on PyPi through <code>pip</code>.</p> <pre><code>pip install qadence-model\n</code></pre>"},{"location":"#install-from-source","title":"Install from source","text":"<p>All Pasqal quantum libraries require Python &gt;=3.9. For development, the preferred method to install this package is to use <code>hatch</code>. You can install from source by cloning this repository and run:</p> <pre><code>python -m pip install hatch\npython -m hatch -v shell\n\n# execute any script using the library\npython my_script.py\n</code></pre> <p>Alternatively, you can also:</p> <ul> <li>install with <code>pip</code> in development mode by simply running <code>pip install -e .</code>. Notice that in this way   you will install all the dependencies, including extras.</li> <li>install it with <code>conda</code> by simply using <code>pip</code> inside the Conda environment.</li> </ul>"},{"location":"#develop","title":"Develop","text":"<p>When developing the package, the recommended way is to create a virtual environment with <code>hatch</code> as shown above:</p> <pre><code>python -m pip install hatch\npython -m hatch -v shell\n</code></pre> <p>When inside the shell with development dependencies, install first the pre-commit hook: <pre><code>pre-commit install\n</code></pre></p> <p>In this way, you will get automatic linting and formatting every time you commit new code. Do not forget to run the unit test suite by simply running the <code>pytest</code> command.</p> <p>If you do not want to get into the Hatch shell, you can alternatively do the following:</p> <pre><code>python -m pip install hatch\npython -m hatch -v shell\n\n# install the pre-commit\npython -m hatch run pre-commit install\n\n# commit some code\npython -m hatch run git commit -m \"My awesome commit\"\n\n# run the unit tests suite\npython -m hatch run pytest\n</code></pre>"},{"location":"#document","title":"Document","text":"<p>You can improve the documentation of the package by editing this file for the landing page or adding new markdown or Jupyter notebooks to the <code>docs/</code> folder in the root of the project. In order to modify the table of contents, edit the <code>mkdocs.yml</code> file in the root of the project.</p> <p>In order to build and serve the documentation locally, you can use <code>hatch</code> with the right environment:</p> <pre><code>python -m hatch -v run docs:build\npython -m hatch -v run docs:serve\n</code></pre> <p>If you don't want to use <code>hatch</code>, just check into your favorite virtual environment and execute the following commands:</p> <pre><code>python -m pip install -r docs/requirements.txt\nmkdocs build\nmkdocs serve\n</code></pre>"},{"location":"CODE_OF_CONDUCT/","title":"CODE OF CONDUCT","text":"<p>Code of Conduct</p>"},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CONTRIBUTING/","title":"How to contribute","text":"<p>We're grateful for your interest in participating in qadence-model. Please follow our guidelines to ensure a smooth contribution process.</p>"},{"location":"CONTRIBUTING/#reporting-an-issue-or-proposing-a-feature","title":"Reporting an issue or proposing a feature","text":"<p>Your course of action will depend on your objective, but generally, you should start by creating an issue. If you've discovered a bug or have a feature you'd like to see added to qadence-model, feel free to create an issue on qadence-hub GitHub issue tracker. Here are some steps to take:</p> <ol> <li>Quickly search the existing issues using relevant keywords to ensure your issue hasn't been addressed already.</li> <li> <p>If your issue is not listed, create a new one. Try to be as detailed and clear as possible in your description.</p> </li> <li> <p>If you're merely suggesting an improvement or reporting a bug, that's already excellent! We thank you for it. Your issue will be listed and, hopefully, addressed at some point.</p> </li> <li>However, if you're willing to be the one solving the issue, that would be even better! In such instances, you would proceed by preparing a Pull Request.</li> </ol>"},{"location":"CONTRIBUTING/#submitting-a-pull-request","title":"Submitting a pull request","text":"<p>We're excited that you're eager to contribute to qadence-model. To contribute, fork the <code>main</code> branch of qadence repository and once you are satisfied with your feature and all the tests pass create a Pull Request.</p> <p>Here's the process for making a contribution:</p> <p>Click the \"Fork\" button at the upper right corner of the repo page to create a new GitHub repo at <code>https://github.com/USERNAME/qadence-hub</code>, where <code>USERNAME</code> is your GitHub ID. Then, <code>cd</code> into the directory where you want to place your new fork and clone it:</p> <pre><code>git clone https://github.com/USERNAME/qadence-hub.git\n</code></pre> <p>Next, navigate to your new qadence fork directory and mark the main qadence repository as the <code>upstream</code>:</p> <pre><code>git remote add upstream https://github.com/pasqal-io/qadence-hub.git\n</code></pre>"},{"location":"CONTRIBUTING/#setting-up-your-development-environment","title":"Setting up your development environment","text":"<p>We recommended to use <code>hatch</code> for managing environments:</p> <p>To develop within qadence, use: <pre><code>pip install hatch\nhatch -v shell\n</code></pre></p> <p>To run qadence tests, use:</p> <pre><code>hatch -e tests run test\n</code></pre> <p>If you don't want to use <code>hatch</code>, you can use the environment manager of your choice (e.g. Conda) and execute the following:</p> <pre><code>pip install pytest\npip install -e .\npytest\n</code></pre>"},{"location":"CONTRIBUTING/#useful-things-for-your-workflow-linting-and-testing","title":"Useful things for your workflow: linting and testing","text":"<p>Use <code>pre-commit</code> to lint your code and run the unit tests before pushing a new commit.</p> <p>Using <code>hatch</code>, it's simply:</p> <pre><code>hatch -e tests run pre-commit run --all-files\nhatch -e tests run test\n</code></pre> <p>Our CI/CD pipeline will also test if the documentation can be built correctly. To test it locally, please run:</p> <pre><code>hatch -e docs run mkdocs build --clean --strict\n</code></pre> <p>Without <code>hatch</code>, <code>pip</code> install those libraries first: \"mkdocs\", \"mkdocs-material\", \"mkdocstrings\", \"mkdocstrings-python\", \"mkdocs-section-index\", \"mkdocs-jupyter\", \"mkdocs-exclude\", \"markdown-exec\"</p> <p>And then:</p> <pre><code> mkdocs build --clean --strict\n</code></pre>"},{"location":"model/qng/","title":"The Quantum Natural Gradient optimizer","text":"<p>Qadence-Model provides a set of optimizers based on quantum information tools, in particular based on the Quantum Fisher Information<sup>1</sup> (QFI). The Quantum Natural Gradient <sup>2</sup> (QNG) is a gradient-based optimizer which uses the QFI matrix to better navigate the optimizer's descent to the minimum. The parameter update rule for the QNG optimizer is written as:</p> \\[ \\theta_{t+1} = \\theta_t - \\eta g^{-1}(\\theta_t)\\nabla \\mathcal{L}(\\theta_t) \\] <p>where \\(g(\\theta)\\) is the Fubiny-Study metric tensor (aka Quantum Geometric Tensor), which is equivalent to the Quantum Fisher Information matrix \\(F(\\theta)\\) up to a constant factor \\(F(\\theta)= 4 g(\\theta)\\). The Quantum Fisher Information can be written as the Hessian of the fidelity of a quantum state:</p> \\[   F_{i j}(\\theta)=-\\left.2 \\frac{\\partial}{\\partial \\theta_i} \\frac{\\partial}{\\partial \\theta_j}\\left|\\left\\langle\\psi\\left(\\theta^{\\prime}\\right) \\mid \\psi(\\theta)\\right\\rangle\\right|^2\\right|_{{\\theta}^{\\prime}=\\theta} \\] <p>However, computing the above expression is a costly operation scaling quadratically with the number of parameters in the variational quantum circuit. It is thus usual to use approximate methods when dealing with the QFI matrix. Qadence-Model provides a SPSA-based implementation of the Quantum Natural Gradient<sup>3</sup>. The SPSA (Simultaneous Perturbation Stochastic Approximation) algorithm is a well known finite differences-based algorithm. QNG-SPSA constructs an iterative approximation to the QFI matrix with a constant number of circuit evaluations that does not scale with the number of parameters. Although the SPSA algorithm outputs a rough approximation of the QFI matrix, the QNG-SPSA has been proven to work well while being a very efficient method due to the constant overhead in circuit evaluations (only 6 extra evaluations per iteration).</p> <p>In this tutorial, we use the QNG and QNG-SPSA optimizers with the Quantum Circuit Learning algorithm, a variational quantum algorithm which uses Quantum Neural Networks as universal function approximators.</p> <p>Keep in mind that only circuit parameters can be optimized with the QNG optimizer, since we can only calculate the QFI matrix of parameters contained in the circuit. If your model holds other trainable, non-circuit parameters, such as scaling or shifting of the input/output, another optimizer must be used for to optimize those parameters. <pre><code>import torch\nfrom torch.utils.data import random_split\nimport random\nimport matplotlib.pyplot as plt\n\nfrom qadence import QuantumCircuit, QNN, FeatureParameter\nfrom qadence import kron, tag, hea, RX, Z, hamiltonian_factory\n\nfrom qadence_model.optimizers import QuantumNaturalGradient\nfrom qadence_model.types import FisherApproximation\n</code></pre> </p> <p>First, we prepare the Quantum Circuit Learning data. In this case we will fit a simple one-dimensional sin(\\(x\\)) function: <pre><code># Ensure reproducibility\nseed = 0\ntorch.manual_seed(seed)\nrandom.seed(seed)\n\n# Create dataset\ndef qcl_training_data(\n    domain: tuple = (0, 2 * torch.pi), n_points: int = 200\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    start, end = domain\n\n    x_rand, _ = torch.sort(torch.DoubleTensor(n_points).uniform_(start, end))\n    y_rand = torch.sin(x_rand)\n\n    return x_rand, y_rand\n\n\nx, y = qcl_training_data()\n\n# random train/test split of the dataset\ntrain_subset, test_subset = random_split(x, [0.75, 0.25])\ntrain_ind = sorted(train_subset.indices)\ntest_ind = sorted(test_subset.indices)\n\nx_train, y_train = x[train_ind], y[train_ind]\nx_test, y_test = x[test_ind], y[test_ind]\n</code></pre> </p> <p>We now create the base Quantum Circuit that we will use with all the optimizers: <pre><code>n_qubits = 3\n\n# create a simple feature map to encode the input data\nfeature_param = FeatureParameter(\"phi\")\nfeature_map = kron(RX(i, feature_param) for i in range(n_qubits))\nfeature_map = tag(feature_map, \"feature_map\")\n\n# create a digital-analog variational ansatz using Qadence convenience constructors\nansatz = hea(n_qubits, depth=n_qubits)\nansatz = tag(ansatz, \"ansatz\")\n\n# Observable\nobservable = hamiltonian_factory(n_qubits, detuning= Z)\n</code></pre> </p>"},{"location":"model/qng/#optimizers","title":"Optimizers","text":"<p>We will experiment with three different optimizers: ADAM, QNG and QNG-SPSA. To train a model with the different optimizers we will create a <code>QuantumModel</code> and reset the values of their variational parameters before each training loop so that all of them have the same starting point.</p> <pre><code># Build circuit and model\ncircuit = QuantumCircuit(n_qubits, feature_map, ansatz)\nmodel = QNN(circuit, [observable])\n\n# Loss function\nmse_loss = torch.nn.MSELoss()\n\n# Initial parameter values\ninitial_params = torch.rand(model.num_vparams)\n</code></pre> <p>We can now train the model with the different corresponding optimizers:</p>"},{"location":"model/qng/#adam","title":"ADAM","text":"<pre><code># Train with ADAM\nn_epochs_adam = 20\nlr_adam = 0.1\n\nmodel.reset_vparams(initial_params)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr_adam)\n\nloss_adam = []\nfor i in range(n_epochs_adam):\n    optimizer.zero_grad()\n    loss = mse_loss(model(values=x_train).squeeze(), y_train.squeeze())\n    loss_adam.append(float(loss))\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"model/qng/#qng","title":"QNG","text":"<p>The way to initialize the <code>QuantumNaturalGradient</code> optimizer in <code>qadence-model</code> is slightly different from other usual Torch optimizers. Normally, one needs to pass a <code>params</code> argument to the optimizer to specify which parameters of the model should be optimized. In the <code>QuantumNaturalGradient</code>, it is assumed that all circuit parameters are to be optimized, whereas the non-circuit parameters will not be optimized. By circuit parameters, we mean parameters that somehow affect the quantum gates of the circuit and therefore influence the final quantum state. Any parameters affecting the observable (such as ouput scaling or shifting) are not considered circuit parameters, as those parameters will not be included in the QFI matrix as they don't affect the final state of the circuit.</p> <p>The <code>QuantumNaturalGradient</code> constructor takes a qadence's <code>QuantumModel</code> as the 'model', and it will automatically identify its circuit and non-circuit parameters. The <code>approximation</code> argument defaults to the SPSA method, however the exact version of the QNG is also implemented and can be used for small circuits (beware of using the exact version for large circuits, as it scales badly). \\(\\beta\\) is a small constant added to the QFI matrix before inversion to ensure numerical stability,</p> \\[(F_{ij} + \\beta \\mathbb{I})^{-1}\\] <p>where \\(\\mathbb{I}\\) is the identify matrix. It is always a good idea to try out different values of \\(\\beta\\) if the training is not converging, which might be due to a too small \\(\\beta\\).</p> <pre><code># Train with QNG\nn_epochs_qng = 20\nlr_qng = 0.1\n\nmodel.reset_vparams(initial_params)\noptimizer = QuantumNaturalGradient(\n    model=model,\n    lr=lr_qng,\n    approximation=FisherApproximation.EXACT,\n    beta=0.1,\n)\n\nloss_qng = []\nfor i in range(n_epochs_qng):\n    optimizer.zero_grad()\n    loss = mse_loss(model(values=x_train).squeeze(), y_train.squeeze())\n    loss_qng.append(float(loss))\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"model/qng/#qng-spsa","title":"QNG-SPSA","text":"<p>The QNG-SPSA optimizer can be constructed similarly to the exact QNG, where now a new argument \\(\\epsilon\\) is used to control the shift used in the finite differences derivatives of the SPSA algorithm.</p> <pre><code># Train with QNG-SPSA\nn_epochs_qng_spsa = 20\nlr_qng_spsa = 0.01\n\nmodel.reset_vparams(initial_params)\noptimizer = QuantumNaturalGradient(\n    model=model,\n    lr=lr_qng_spsa,\n    approximation=FisherApproximation.SPSA,\n    beta=0.1,\n    epsilon=0.01,\n)\n\nloss_qng_spsa = []\nfor i in range(n_epochs_qng_spsa):\n    optimizer.zero_grad()\n    loss = mse_loss(model(values=x_train).squeeze(), y_train.squeeze())\n    loss_qng_spsa.append(float(loss))\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"model/qng/#plotting","title":"Plotting","text":"<p>We now plot the losses corresponding to each of the optimizers: <pre><code># Plot losses\nfig, _ = plt.subplots()\nplt.plot(range(n_epochs_adam), loss_adam, label=\"Adam optimizer\")\nplt.plot(range(n_epochs_qng), loss_qng, label=\"QNG optimizer\")\nplt.plot(range(n_epochs_qng_spsa), loss_qng_spsa, label=\"QNG-SPSA optimizer\")\nplt.legend()\nplt.xlabel(\"Training epochs\")\nplt.ylabel(\"Loss\")\n</code></pre> </p>"},{"location":"model/qng/#references","title":"References","text":"<ol> <li> <p>Meyer J., Information in Noisy Intermediate-Scale Quantum Applications, Quantum 5, 539 (2021) \u21a9</p> </li> <li> <p>Stokes et al., Quantum Natural Gradient, Quantum 4, 269 (2020). \u21a9</p> </li> <li> <p>Gacon et al., Simultaneous Perturbation Stochastic Approximation of the Quantum Fisher Information, Quantum 5, 567 (2021). \u21a9</p> </li> </ol>"},{"location":"model/qnn_config/","title":"QNN Config","text":"<p>In <code>qadence</code>, the <code>QNN</code> is a variational quantum model that can potentially take multi-dimensional input.</p> <p>The <code>QNN</code> class needs a circuit and a list of observables; the number of feature parameters in the input circuit determines the number of input features (i.e. the dimensionality of the classical data given as input) whereas the number of observables determines the number of outputs of the quantum neural network.</p> <p>The circuit has two parts, the feature map and the ansatz. The feature map is responsible for encoding the input data into the quantum state, while the ansatz is responsible for the variational part of the model. In addition, a third part of the QNN is the observables, which is (a list of) operators that are measured at the end of the circuit. In this tutorial, we will see how to do the same using configs.</p> <p>One convenient way to construct these two parts of the model is to use the config classes, namely, <code>FeatureMapConfig</code> and <code>AnsatzConfig</code>. These classes allow you to specify the type of circuit and the parameters of the circuit in a structured way.</p>"},{"location":"model/qnn_config/#defining-the-observable","title":"Defining the Observable","text":"<p>The model output is the expectation value of the defined observable(s). We use the <code>ObservableConfig</code> class to specify the observable.</p> <p>It can be used to create Hamiltonians with 2-qubit interactions and single-qubit detunings. Any Hamiltonian supported by <code>hamiltonian_factory</code> can be specified as an observable. For example, suppose we want to measure the Z operator:</p> <pre><code>from qadence import create_observable, ObservableConfig, Z\n\nobservable_config = ObservableConfig(\n    detuning=Z,\n    interaction = None,\n    scale = 2.0,\n    shift=-1.0,\n)\n\nobservable = create_observable(register=4, config=observable_config)\n</code></pre> %3 cluster_a45549cb3feb4930853819936a9b56a5 b4b169b3c9ea414680e113e930fa19a3 0 55235efcc0704a6e97fcad3e9e9106a7 b4b169b3c9ea414680e113e930fa19a3--55235efcc0704a6e97fcad3e9e9106a7 389dcc47826c41e4a716c3694252d532 1 20538ea2542b47a388ce7e3497a5c8a1 55235efcc0704a6e97fcad3e9e9106a7--20538ea2542b47a388ce7e3497a5c8a1 e7646b63bcf34ac6bb5ab00c697b655c 9125daff5b5944cb91a4331aa7e2c517 AddBlock 389dcc47826c41e4a716c3694252d532--9125daff5b5944cb91a4331aa7e2c517 7a76f1ec80d5409ca29925c3d4cfc96c 2 9125daff5b5944cb91a4331aa7e2c517--e7646b63bcf34ac6bb5ab00c697b655c f4c342bcc1764a4ab6d262595586a335 d38055e72a0843ec8613a22c73eab639 7a76f1ec80d5409ca29925c3d4cfc96c--d38055e72a0843ec8613a22c73eab639 7f40f08b462c45b1816c86a6b43252b4 3 d38055e72a0843ec8613a22c73eab639--f4c342bcc1764a4ab6d262595586a335 138a46c323324d729c2fba461859a60b 7b82e3d7070d408b8c547ee0811e2e1d 7f40f08b462c45b1816c86a6b43252b4--7b82e3d7070d408b8c547ee0811e2e1d 7b82e3d7070d408b8c547ee0811e2e1d--138a46c323324d729c2fba461859a60b"},{"location":"model/qnn_config/#defining-the-feature-map","title":"Defining the Feature Map","text":"<p>Let us say we want to build a 4-qubit QNN that takes two inputs, namely, the \\(x\\) and the \\(y\\) coordinates of a point in the plane. We can use the <code>FeatureMapConfig</code> class to specify the feature map.</p> <pre><code>from qadence import BasisSet, chain, create_fm_blocks, FeatureMapConfig, ReuploadScaling\n\nfm_config = FeatureMapConfig(\n    num_features=2,\n    inputs = [\"x\", \"y\"],\n    basis_set=BasisSet.CHEBYSHEV,\n    reupload_scaling=ReuploadScaling.TOWER,\n    feature_range={\n        \"x\": (-1.0, 1.0),\n        \"y\": (0.0, 1.0),\n    },\n)\n\nfm_blocks = create_fm_blocks(register=4, config=fm_config)\nfeature_map = chain(*fm_blocks)\n</code></pre> %3 cluster_a8f4453d22774e519b7946be49e38e90 Tower Chebyshev FM cluster_22ff185312014850a6ddadd5fd50d0a7 Tower Chebyshev FM cd2939e031084fe88c55a2ca1077ac21 0 e607b574831343c4b42de3e367d0025f RX(1.0*acos(x)) cd2939e031084fe88c55a2ca1077ac21--e607b574831343c4b42de3e367d0025f a247aacc29984d15a48aebf25c6bf999 1 b41e0ef7c8f84291b1d73c3ea704990b e607b574831343c4b42de3e367d0025f--b41e0ef7c8f84291b1d73c3ea704990b 9fc49f0f025f47a98f2f8db6eb1e97ed c1ae8420b4c147088434594115b8767a RX(2.0*acos(x)) a247aacc29984d15a48aebf25c6bf999--c1ae8420b4c147088434594115b8767a 82aa907f3133446dab385a83cfca723e 2 c1ae8420b4c147088434594115b8767a--9fc49f0f025f47a98f2f8db6eb1e97ed cfe1da92c68d466393b1e6c48dc9dc90 dcaebffaa02e4555b9519ac89c1490a2 RX(1.0*acos(2.0*y - 1.0)) 82aa907f3133446dab385a83cfca723e--dcaebffaa02e4555b9519ac89c1490a2 61ae447d4a454af4a601389968e51242 3 dcaebffaa02e4555b9519ac89c1490a2--cfe1da92c68d466393b1e6c48dc9dc90 0dff8653d35b42638b4122a5372b9f2b b2c350a1d5474c04bb96bfa8ebb94af4 RX(2.0*acos(2.0*y - 1.0)) 61ae447d4a454af4a601389968e51242--b2c350a1d5474c04bb96bfa8ebb94af4 b2c350a1d5474c04bb96bfa8ebb94af4--0dff8653d35b42638b4122a5372b9f2b <p>We have specified that the feature map should take two features, and have named the <code>FeatureParameter</code> \"x\" and \"y\" respectively. Both these parameters are encoded using the Chebyshev basis set, and the reupload scaling is set to <code>ReuploadScaling.TOWER</code>. One can optionally add the basis and the reupload scaling for each parameter separately.</p> <p>The <code>feature_range</code> parameter is a dictionary that specifies the range of values that each feature comes from. This is useful for scaling the input data to the range that the encoding function can handle. In default case, this range is mapped to the target range of the Chebyshev basis set which is \\([-1, 1]\\). One can also specify the target range for each feature separately..</p>"},{"location":"model/qnn_config/#defining-the-ansatz","title":"Defining the Ansatz","text":"<p>The next part of the QNN is the ansatz. We use <code>AnsatzConfig</code> class to specify the type of ansatz.</p> <p>Let us say, we want to follow this feature map with 2 layers of hardware efficient ansatz.</p> <pre><code>from qadence import AnsatzConfig, AnsatzType, create_ansatz, Strategy\n\nansatz_config = AnsatzConfig(\n    depth=2,\n    ansatz_type=AnsatzType.HEA,\n    ansatz_strategy=Strategy.DIGITAL,\n)\n\nansatz = create_ansatz(register=4, config=ansatz_config)\n</code></pre> %3 4e38f91f472546499f25f35d49eafb50 0 1d0eb2c0ec5047158eae8e381d8743a1 RX(theta\u2080) 4e38f91f472546499f25f35d49eafb50--1d0eb2c0ec5047158eae8e381d8743a1 c62c0bfa2142443e8ca6d366d1fdf4f6 1 9bba0ee19e954b2e8855618bce0ee592 RY(theta\u2084) 1d0eb2c0ec5047158eae8e381d8743a1--9bba0ee19e954b2e8855618bce0ee592 c7c645484721498193083f3481ad3601 RX(theta\u2088) 9bba0ee19e954b2e8855618bce0ee592--c7c645484721498193083f3481ad3601 b73f68d472c841d8aedda18eb3e1daac c7c645484721498193083f3481ad3601--b73f68d472c841d8aedda18eb3e1daac 6941ca598bdb472c876b7b92429130ba b73f68d472c841d8aedda18eb3e1daac--6941ca598bdb472c876b7b92429130ba cf1b5fc96a9948fa975db31346fb2b94 RX(theta\u2081\u2082) 6941ca598bdb472c876b7b92429130ba--cf1b5fc96a9948fa975db31346fb2b94 f2d89feb4ae24b0e920c6b8ef25bb8da RY(theta\u2081\u2086) cf1b5fc96a9948fa975db31346fb2b94--f2d89feb4ae24b0e920c6b8ef25bb8da f0e440337e7e4d83bff2e4bd419f7312 RX(theta\u2082\u2080) f2d89feb4ae24b0e920c6b8ef25bb8da--f0e440337e7e4d83bff2e4bd419f7312 04d14dca14cb4158b42482ac7beb3133 f0e440337e7e4d83bff2e4bd419f7312--04d14dca14cb4158b42482ac7beb3133 7afb490e9ae04c4aa9b489be580b44b4 04d14dca14cb4158b42482ac7beb3133--7afb490e9ae04c4aa9b489be580b44b4 2fe74ac6aafe4bf599b1e6aeece5cf10 7afb490e9ae04c4aa9b489be580b44b4--2fe74ac6aafe4bf599b1e6aeece5cf10 ea06cc5d7501445ab1ebfc23efc33907 761b34759e3d41d3abf124b688b728a9 RX(theta\u2081) c62c0bfa2142443e8ca6d366d1fdf4f6--761b34759e3d41d3abf124b688b728a9 fe5b2cffa4f7444d8ab53e2222d4a958 2 a7fbae7e9f8546f59366dd309a55bec5 RY(theta\u2085) 761b34759e3d41d3abf124b688b728a9--a7fbae7e9f8546f59366dd309a55bec5 5f96dbbc6415483d8c9781354d04827d RX(theta\u2089) a7fbae7e9f8546f59366dd309a55bec5--5f96dbbc6415483d8c9781354d04827d 8478122c81ca4959b70e74d23bcb135e X 5f96dbbc6415483d8c9781354d04827d--8478122c81ca4959b70e74d23bcb135e 8478122c81ca4959b70e74d23bcb135e--b73f68d472c841d8aedda18eb3e1daac 97ccaa3566c74930b6fd4279595d36c9 8478122c81ca4959b70e74d23bcb135e--97ccaa3566c74930b6fd4279595d36c9 c9afcfe984b34ead909528890a4cac4a RX(theta\u2081\u2083) 97ccaa3566c74930b6fd4279595d36c9--c9afcfe984b34ead909528890a4cac4a 910c4ca49e8d4e70842b0fe26a6d55a8 RY(theta\u2081\u2087) c9afcfe984b34ead909528890a4cac4a--910c4ca49e8d4e70842b0fe26a6d55a8 2ad42b9db4b54f598bf8c099bdf21bd4 RX(theta\u2082\u2081) 910c4ca49e8d4e70842b0fe26a6d55a8--2ad42b9db4b54f598bf8c099bdf21bd4 23b6cf7353a84660862cbd2e2f43c882 X 2ad42b9db4b54f598bf8c099bdf21bd4--23b6cf7353a84660862cbd2e2f43c882 23b6cf7353a84660862cbd2e2f43c882--04d14dca14cb4158b42482ac7beb3133 abeccee150b84f2ebe66440f80074b1d 23b6cf7353a84660862cbd2e2f43c882--abeccee150b84f2ebe66440f80074b1d abeccee150b84f2ebe66440f80074b1d--ea06cc5d7501445ab1ebfc23efc33907 f097505f2abe4ec7bb9f012f4115e5db d6a66c09f7c24cca8e664595eaba875a RX(theta\u2082) fe5b2cffa4f7444d8ab53e2222d4a958--d6a66c09f7c24cca8e664595eaba875a 7f79f2e2f56e4684afb8272a06e98ae7 3 b8067b0a52534ea7b69ce9661ad91b10 RY(theta\u2086) d6a66c09f7c24cca8e664595eaba875a--b8067b0a52534ea7b69ce9661ad91b10 66245227b9664894ba805c8d61911931 RX(theta\u2081\u2080) b8067b0a52534ea7b69ce9661ad91b10--66245227b9664894ba805c8d61911931 525bc5f0563d48469618885408e3f8ea 66245227b9664894ba805c8d61911931--525bc5f0563d48469618885408e3f8ea e26900527d934693b57e4b9c9ad413e9 X 525bc5f0563d48469618885408e3f8ea--e26900527d934693b57e4b9c9ad413e9 e26900527d934693b57e4b9c9ad413e9--97ccaa3566c74930b6fd4279595d36c9 aca0f2d257a84f82828fedfdd86ca2a3 RX(theta\u2081\u2084) e26900527d934693b57e4b9c9ad413e9--aca0f2d257a84f82828fedfdd86ca2a3 d20aa8151622418f82002a2509c3245c RY(theta\u2081\u2088) aca0f2d257a84f82828fedfdd86ca2a3--d20aa8151622418f82002a2509c3245c 4a01696101664ef19496f70b9fabad1e RX(theta\u2082\u2082) d20aa8151622418f82002a2509c3245c--4a01696101664ef19496f70b9fabad1e 4da877c211b141eb8c09594c7ca407d0 4a01696101664ef19496f70b9fabad1e--4da877c211b141eb8c09594c7ca407d0 6940b9ed4ea640008ac80025a3d88051 X 4da877c211b141eb8c09594c7ca407d0--6940b9ed4ea640008ac80025a3d88051 6940b9ed4ea640008ac80025a3d88051--abeccee150b84f2ebe66440f80074b1d 6940b9ed4ea640008ac80025a3d88051--f097505f2abe4ec7bb9f012f4115e5db 52efcd8fcdd048409d4fd46294c0d64d 6d52246c354a4d77a93e5b0b35a28222 RX(theta\u2083) 7f79f2e2f56e4684afb8272a06e98ae7--6d52246c354a4d77a93e5b0b35a28222 e6792a0c58c943678deae12e96c1ebe0 RY(theta\u2087) 6d52246c354a4d77a93e5b0b35a28222--e6792a0c58c943678deae12e96c1ebe0 fd0c35cca6fe47578918702807c9a9dd RX(theta\u2081\u2081) e6792a0c58c943678deae12e96c1ebe0--fd0c35cca6fe47578918702807c9a9dd d412c8a75d9841e4903f53531dfe35c9 X fd0c35cca6fe47578918702807c9a9dd--d412c8a75d9841e4903f53531dfe35c9 d412c8a75d9841e4903f53531dfe35c9--525bc5f0563d48469618885408e3f8ea 0cfff1e7cbfe4e279382fa81efbd244a d412c8a75d9841e4903f53531dfe35c9--0cfff1e7cbfe4e279382fa81efbd244a cf77d5efc3ff4051add3c672de3ea43b RX(theta\u2081\u2085) 0cfff1e7cbfe4e279382fa81efbd244a--cf77d5efc3ff4051add3c672de3ea43b f288fd9b0cc5403cbcbf8fa8179f533d RY(theta\u2081\u2089) cf77d5efc3ff4051add3c672de3ea43b--f288fd9b0cc5403cbcbf8fa8179f533d 2ae0b20784e84d6abf593f61f1305523 RX(theta\u2082\u2083) f288fd9b0cc5403cbcbf8fa8179f533d--2ae0b20784e84d6abf593f61f1305523 9f2d9f0d5414477a8af5d88321829758 X 2ae0b20784e84d6abf593f61f1305523--9f2d9f0d5414477a8af5d88321829758 9f2d9f0d5414477a8af5d88321829758--4da877c211b141eb8c09594c7ca407d0 2e7d43620e81490d8934502f9e27c9c7 9f2d9f0d5414477a8af5d88321829758--2e7d43620e81490d8934502f9e27c9c7 2e7d43620e81490d8934502f9e27c9c7--52efcd8fcdd048409d4fd46294c0d64d <p>We have specified that the ansatz should have a depth of 2, and the ansatz type is \"hea\" (Hardware Efficient Ansatz). The ansatz strategy is set to \"digital\", which means digital gates are being used. One could alternatively use \"analog\" or \"rydberg\" as the ansatz strategy.</p>"},{"location":"model/qnn_config/#defining-the-qnn-from-the-configs","title":"Defining the QNN from the Configs","text":"<p>To build the QNN, we can now use the <code>QNN</code> class as a <code>QuantumModel</code> subtype. In addition to the feature map, ansatz and the observable configs, we can also specify options such as the <code>backend</code>, <code>diff_mode</code>, etc.</p> <pre><code>from qadence import BackendName, DiffMode, QNN, ObservableConfig, Z\n\nobservable_config = ObservableConfig(\n    detuning=Z,\n    interaction = None,\n    scale = 2.0,\n    shift=-1.0,\n)\n\nqnn = QNN.from_configs(\n    register=4,\n    obs_config=observable_config,\n    fm_config=fm_config,\n    ansatz_config=ansatz_config,\n    backend=BackendName.PYQTORCH,\n    diff_mode=DiffMode.AD,\n)\n</code></pre> %3 cluster_23b1f148d627479389ac1ed91dbf2d71 Obs. cluster_1b799e98a78f43bd8147f94ee2ef4450 cluster_746786b9ecef4c488d90aa00c3903940 Tower Chebyshev FM cluster_eaa39817f02943909e1158b83c54a42a Tower Chebyshev FM cluster_918584e3294648a5972c787c71b2367c HEA 6cb28039e439460c841143b096bc8420 0 f610f975ea0640fcbdc36ebf32c15f7a RX(1.0*acos(x)) 6cb28039e439460c841143b096bc8420--f610f975ea0640fcbdc36ebf32c15f7a 3e01d54ad35a40b19c3c6930c3567d28 1 2c1ef96f6a4240209a49ae570a94d874 RX(theta\u2080) f610f975ea0640fcbdc36ebf32c15f7a--2c1ef96f6a4240209a49ae570a94d874 081d93d72e9a4e58b25df580be3e2a93 RY(theta\u2084) 2c1ef96f6a4240209a49ae570a94d874--081d93d72e9a4e58b25df580be3e2a93 bc10532365744dbb96332498bd14ff2c RX(theta\u2088) 081d93d72e9a4e58b25df580be3e2a93--bc10532365744dbb96332498bd14ff2c ebb3531c474244308e984397ad0d2047 bc10532365744dbb96332498bd14ff2c--ebb3531c474244308e984397ad0d2047 ef59ec64b54a43038070931b525c7f41 ebb3531c474244308e984397ad0d2047--ef59ec64b54a43038070931b525c7f41 3145cc8dd04743f18ac29dea11f0e365 RX(theta\u2081\u2082) ef59ec64b54a43038070931b525c7f41--3145cc8dd04743f18ac29dea11f0e365 4d05f8615b0a458db3942da179f39e26 RY(theta\u2081\u2086) 3145cc8dd04743f18ac29dea11f0e365--4d05f8615b0a458db3942da179f39e26 d650d0679ff84354a0af4871599706be RX(theta\u2082\u2080) 4d05f8615b0a458db3942da179f39e26--d650d0679ff84354a0af4871599706be e9d6ced9ec6a4a96875d4cb0bdfc2057 d650d0679ff84354a0af4871599706be--e9d6ced9ec6a4a96875d4cb0bdfc2057 7b80237d86ea48ffb0527c5e0dd101ce e9d6ced9ec6a4a96875d4cb0bdfc2057--7b80237d86ea48ffb0527c5e0dd101ce 2ff082557e3641f7b6de96042699c9ad 7b80237d86ea48ffb0527c5e0dd101ce--2ff082557e3641f7b6de96042699c9ad a5da1292fa5d4de189e8d1eef0c73b33 2ff082557e3641f7b6de96042699c9ad--a5da1292fa5d4de189e8d1eef0c73b33 0af9badad42a446eaf00f9b5d250e8aa 5c50eba3ad8f40e1bc93a70b92be38af RX(2.0*acos(x)) 3e01d54ad35a40b19c3c6930c3567d28--5c50eba3ad8f40e1bc93a70b92be38af 61e1940784804d4bb2ba4c0e11084454 2 8cec43e1e03244fc8bcdf95ca9941d1a RX(theta\u2081) 5c50eba3ad8f40e1bc93a70b92be38af--8cec43e1e03244fc8bcdf95ca9941d1a ff4c7da569504d8bbc2bc1b967ee773c RY(theta\u2085) 8cec43e1e03244fc8bcdf95ca9941d1a--ff4c7da569504d8bbc2bc1b967ee773c 147b4c201c7d446481258fe18612c4be RX(theta\u2089) ff4c7da569504d8bbc2bc1b967ee773c--147b4c201c7d446481258fe18612c4be e71652f8049c4b63b47e62235fdbbc35 X 147b4c201c7d446481258fe18612c4be--e71652f8049c4b63b47e62235fdbbc35 e71652f8049c4b63b47e62235fdbbc35--ebb3531c474244308e984397ad0d2047 809c2f68126148bbba45221896fcdc4a e71652f8049c4b63b47e62235fdbbc35--809c2f68126148bbba45221896fcdc4a 2b6127b891d04a3f95f9d872f3a568ed RX(theta\u2081\u2083) 809c2f68126148bbba45221896fcdc4a--2b6127b891d04a3f95f9d872f3a568ed 8663371a4d33430d90e82149df158311 RY(theta\u2081\u2087) 2b6127b891d04a3f95f9d872f3a568ed--8663371a4d33430d90e82149df158311 3afb0161c013457cbcf2a6260076c707 RX(theta\u2082\u2081) 8663371a4d33430d90e82149df158311--3afb0161c013457cbcf2a6260076c707 1c083c0beba04b12b1b61e4dde9d97cf X 3afb0161c013457cbcf2a6260076c707--1c083c0beba04b12b1b61e4dde9d97cf 1c083c0beba04b12b1b61e4dde9d97cf--e9d6ced9ec6a4a96875d4cb0bdfc2057 9813ab31f8be4990bc85c0fee714d8f8 1c083c0beba04b12b1b61e4dde9d97cf--9813ab31f8be4990bc85c0fee714d8f8 3609e6fa0b16493199cb905e56bbbd50 AddBlock 9813ab31f8be4990bc85c0fee714d8f8--3609e6fa0b16493199cb905e56bbbd50 3609e6fa0b16493199cb905e56bbbd50--0af9badad42a446eaf00f9b5d250e8aa 49563c8013f1440cb170a9db1be2b5db 7418dbb423864174985c6c6e57afe5c8 RX(1.0*acos(2.0*y - 1.0)) 61e1940784804d4bb2ba4c0e11084454--7418dbb423864174985c6c6e57afe5c8 bb67ce7ff1ed48df902d6a200705c2aa 3 48d0b0ec72cd4b369cbb1fdf61af944f RX(theta\u2082) 7418dbb423864174985c6c6e57afe5c8--48d0b0ec72cd4b369cbb1fdf61af944f eb459ddcfda1493c8a6ec0c1f3ad1ef6 RY(theta\u2086) 48d0b0ec72cd4b369cbb1fdf61af944f--eb459ddcfda1493c8a6ec0c1f3ad1ef6 3e01f71ddaff48a2935ef4971bdd845f RX(theta\u2081\u2080) eb459ddcfda1493c8a6ec0c1f3ad1ef6--3e01f71ddaff48a2935ef4971bdd845f e2154f6561f24c13b5712b0c76b67114 3e01f71ddaff48a2935ef4971bdd845f--e2154f6561f24c13b5712b0c76b67114 be253ee0ead14a21a5298d43eb394dac X e2154f6561f24c13b5712b0c76b67114--be253ee0ead14a21a5298d43eb394dac be253ee0ead14a21a5298d43eb394dac--809c2f68126148bbba45221896fcdc4a c55e7fad31b04713b6d9172eb1669584 RX(theta\u2081\u2084) be253ee0ead14a21a5298d43eb394dac--c55e7fad31b04713b6d9172eb1669584 83b27c2c2f1245e9936ecf9923e2db25 RY(theta\u2081\u2088) c55e7fad31b04713b6d9172eb1669584--83b27c2c2f1245e9936ecf9923e2db25 fa96e6b72da1475e96eb76f70adfc891 RX(theta\u2082\u2082) 83b27c2c2f1245e9936ecf9923e2db25--fa96e6b72da1475e96eb76f70adfc891 8cd0cffb601342fc902c04bfccc737be fa96e6b72da1475e96eb76f70adfc891--8cd0cffb601342fc902c04bfccc737be 5e102e09ff324e979758bc18eb14ce52 X 8cd0cffb601342fc902c04bfccc737be--5e102e09ff324e979758bc18eb14ce52 5e102e09ff324e979758bc18eb14ce52--9813ab31f8be4990bc85c0fee714d8f8 7fd26f6d4de94362a6046b67690b9c68 5e102e09ff324e979758bc18eb14ce52--7fd26f6d4de94362a6046b67690b9c68 7fd26f6d4de94362a6046b67690b9c68--49563c8013f1440cb170a9db1be2b5db a8e9418c401b45a9930224d7099d375c a6d27c1349b845dd949b758ad445c4ab RX(2.0*acos(2.0*y - 1.0)) bb67ce7ff1ed48df902d6a200705c2aa--a6d27c1349b845dd949b758ad445c4ab a84a5c7ae2414e60954ecadcc6f4e2d2 RX(theta\u2083) a6d27c1349b845dd949b758ad445c4ab--a84a5c7ae2414e60954ecadcc6f4e2d2 9466669b68344d8a9634f21f00ec9edb RY(theta\u2087) a84a5c7ae2414e60954ecadcc6f4e2d2--9466669b68344d8a9634f21f00ec9edb d9e67c5fa8e34560a30f4f1007450f88 RX(theta\u2081\u2081) 9466669b68344d8a9634f21f00ec9edb--d9e67c5fa8e34560a30f4f1007450f88 23c6ced4039740fd9c14e9e9e0af4b00 X d9e67c5fa8e34560a30f4f1007450f88--23c6ced4039740fd9c14e9e9e0af4b00 23c6ced4039740fd9c14e9e9e0af4b00--e2154f6561f24c13b5712b0c76b67114 97dd75401e424f569d62ec7c53ec0693 23c6ced4039740fd9c14e9e9e0af4b00--97dd75401e424f569d62ec7c53ec0693 539f4fc7b3f540a68c08b2e6fcd5c6b8 RX(theta\u2081\u2085) 97dd75401e424f569d62ec7c53ec0693--539f4fc7b3f540a68c08b2e6fcd5c6b8 af43718eff8b49f79c5b7b43d759c603 RY(theta\u2081\u2089) 539f4fc7b3f540a68c08b2e6fcd5c6b8--af43718eff8b49f79c5b7b43d759c603 5fd79dd0e1fd4a55a98395f4127a9c4d RX(theta\u2082\u2083) af43718eff8b49f79c5b7b43d759c603--5fd79dd0e1fd4a55a98395f4127a9c4d 1002e32899eb4042bae143449b1e4b87 X 5fd79dd0e1fd4a55a98395f4127a9c4d--1002e32899eb4042bae143449b1e4b87 1002e32899eb4042bae143449b1e4b87--8cd0cffb601342fc902c04bfccc737be 1f5f36e8859743db91030e92fe3e0861 1002e32899eb4042bae143449b1e4b87--1f5f36e8859743db91030e92fe3e0861 5c6b5cbcb472407b9865a953b31a410e 1f5f36e8859743db91030e92fe3e0861--5c6b5cbcb472407b9865a953b31a410e 5c6b5cbcb472407b9865a953b31a410e--a8e9418c401b45a9930224d7099d375c"}]}